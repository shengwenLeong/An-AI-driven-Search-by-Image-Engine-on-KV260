#########################################################################
#input layer for deployment

#name: "miniGoogleNet for CIFAR10, model 1"
#layer {
#  name: "data"
#  type: "Input"
#  top: "data"
#  input_param { shape: { dim: 1 dim: 3 dim: 32 dim: 32 } }
#}

#########################################################################
# layers for TRAIN and TEST

name: "miniGoogleNet for CIFAR10, model 3"
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: true
    crop_size: 32
    #mean_file: "../../data/ilsvrc12/imagenet_mean.binaryproto"
        mean_value: 104
        mean_value: 117
        mean_value: 123
  }
  data_param {
    source: "../../data/cifar10/cifar10_train_leveldb"
    batch_size: 32
  }
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    crop_size: 32
    #mean_file: "../../data/ilsvrc12/imagenet_mean.binaryproto"
        mean_value: 104
        mean_value: 117
        mean_value: 123

  }
  data_param {
    source: "../../data/cifar10/cifar10_val_leveldb"
    batch_size: 32
  }
}


############################################################
#L1 MiniGoogLeNet.conv_module(inputs, 96, 3, 3, (1, 1), chanDim)
############################################################
layer {
  name: "conv1/3x3_s1"
  type: "Convolution"
  bottom: "data"
  top: "conv1/3x3_s1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
      std: 0.1
    }
    bias_filler {
      type: "constant"
      value: 0.2
    }
  }
}
layer {
  name: "conv1/bn1"
  type: "BatchNorm"
  bottom: "conv1/3x3_s1"
  top: "conv1/3x3_s1"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "conv1/scale1"
  type: "Scale"
  bottom: "conv1/3x3_s1"
  top: "conv1/3x3_s1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv1/relu1"
  type: "ReLU"
  bottom: "conv1/3x3_s1"
  top: "conv1/3x3_s1"
}
####################################################################
# L2 MiniGoogLeNet.inception_module(x, 32, 32, chanDim)
####################################################################
layer {
  name: "inception_2a/1x1"
  type: "Convolution"
  bottom: "conv1/3x3_s1"
  top: "inception_2a/1x1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    kernel_size: 1
    weight_filler {
      type: "xavier"
      std: 0.03
    }
    bias_filler {
      type: "constant"
      value: 0.2
    }
  }
}
layer {
  name: "inception_2a/1x1/bn1"
  type: "BatchNorm"
  bottom: "inception_2a/1x1"
  top: "inception_2a/1x1"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "inception_2a/1x1/scale1"
  type: "Scale"
  bottom: "inception_2a/1x1"
  top: "inception_2a/1x1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "inception_2a/1x1/relu1"
  type: "ReLU"
  bottom: "inception_2a/1x1"
  top: "inception_2a/1x1"
}

layer {
  name: "inception_2a/3x3"
  type: "Convolution"
  bottom: "conv1/3x3_s1"
  top: "inception_2a/3x3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    kernel_size: 3
    pad: 1
    weight_filler {
      type: "xavier"
      std: 0.03
    }
    bias_filler {
      type: "constant"
      value: 0.2
    }
  }
}
layer {
  name: "inception_2a/3x3/bn1"
  type: "BatchNorm"
  bottom: "inception_2a/3x3"
  top: "inception_2a/3x3"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "inception_2a/3x3/scale1"
  type: "Scale"
  bottom: "inception_2a/3x3"
  top: "inception_2a/3x3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "inception_2a/3x3/relu1"
  type: "ReLU"
  bottom: "inception_2a/3x3"
  top: "inception_2a/3x3"
}

layer {
  name: "inception_2a/output"
  type: "Concat"
  bottom: "inception_2a/1x1"
  bottom: "inception_2a/3x3"
  top: "inception_2a/output"
}
########################################################################
# L3 MiniGoogLeNet.inception_module(x, 32, 48, chanDim)
########################################################################
layer {
  name: "inception_3a/1x1"
  type: "Convolution"
  bottom: "inception_2a/output"
  top: "inception_3a/1x1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    kernel_size: 1
    weight_filler {
      type: "xavier"
      std: 0.03
    }
    bias_filler {
      type: "constant"
      value: 0.2
    }
  }
}
layer {
  name: "inception_3a/1x1/bn1"
  type: "BatchNorm"
  bottom: "inception_3a/1x1"
  top: "inception_3a/1x1"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "inception_3a/1x1/scale1"
  type: "Scale"
  bottom: "inception_3a/1x1"
  top: "inception_3a/1x1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "inception_3a/1x1/relu1"
  type: "ReLU"
  bottom: "inception_3a/1x1"
  top: "inception_3a/1x1"
}

layer {
  name: "inception_3a/3x3"
  type: "Convolution"
  bottom: "inception_2a/output"
  top: "inception_3a/3x3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 48
    kernel_size: 3
    pad: 1
    weight_filler {
      type: "xavier"
      std: 0.03
    }
    bias_filler {
      type: "constant"
      value: 0.2
    }
  }
}
layer {
  name: "inception_3a/3x3/bn1"
  type: "BatchNorm"
  bottom: "inception_3a/3x3"
  top: "inception_3a/3x3"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "inception_3a/3x3/scale1"
  type: "Scale"
  bottom: "inception_3a/3x3"
  top: "inception_3a/3x3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "inception_3a/3x3/relu1"
  type: "ReLU"
  bottom: "inception_3a/3x3"
  top: "inception_3a/3x3"
}

layer {
  name: "inception_3a/output"
  type: "Concat"
  bottom: "inception_3a/1x1"
  bottom: "inception_3a/3x3"
  top: "inception_3a/output"
}

########################################################################
# downample module
#L4 MiniGoogLeNet.downsample_module(x, 80, chanDim)
########################################################################

layer {
  name: "downsample_4/3x3_s2"
  type: "Convolution"
  bottom: "inception_3a/output"
  top: "downsample_4/3x3_s2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 80
    kernel_size: 3
    stride:2
    pad: 1
    weight_filler {
      type: "xavier"
      std: 0.03
    }
    bias_filler {
      type: "constant"
      value: 0.2
    }
  }
}
layer {
  name: "downsample_4/3x3_s2/bn1"
  type: "BatchNorm"
  bottom: "downsample_4/3x3_s2"
  top: "downsample_4/3x3_s2"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "downsample_4/3x3_s2/scale1"
  type: "Scale"
  bottom: "downsample_4/3x3_s2"
  top: "downsample_4/3x3_s2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "downsample_4/3x3_s2/relu1"
  type: "ReLU"
  bottom: "downsample_4/3x3_s2"
  top: "downsample_4/3x3_s2"
}
layer {
  name: "downsample_4/pool_s2"
  type: "Pooling"
  bottom: "inception_3a/output"
  top: "downsample_4/pool_s2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    #pad: 1
  }
}
layer {
  name: "downsample_4/output"
  type: "Concat"
  bottom: "downsample_4/3x3_s2"
  bottom: "downsample_4/pool_s2"
  top: "downsample_4/output"
}

########################################################################
# L5 MiniGoogLeNet.inception_module(x, 112, 48, chanDim)
########################################################################
layer {
  name: "inception_5a/1x1"
  type: "Convolution"
  bottom: "downsample_4/output"
  top: "inception_5a/1x1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 112
    kernel_size: 1
    weight_filler {
      type: "xavier"
      std: 0.03
    }
    bias_filler {
      type: "constant"
      value: 0.2
    }
  }
}
layer {
  name: "inception_5a/1x1/bn1"
  type: "BatchNorm"
  bottom: "inception_5a/1x1"
  top: "inception_5a/1x1"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "inception_5a/1x1/scale1"
  type: "Scale"
  bottom: "inception_5a/1x1"
  top: "inception_5a/1x1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "inception_5a/1x1/relu1"
  type: "ReLU"
  bottom: "inception_5a/1x1"
  top: "inception_5a/1x1"
}

layer {
  name: "inception_5a/3x3"
  type: "Convolution"
  bottom: "downsample_4/output"
  top: "inception_5a/3x3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 48
    kernel_size: 3
    pad: 1
    weight_filler {
      type: "xavier"
      std: 0.03
    }
    bias_filler {
      type: "constant"
      value: 0.2
    }
  }
}
layer {
  name: "inception_5a/3x3/bn1"
  type: "BatchNorm"
  bottom: "inception_5a/3x3"
  top: "inception_5a/3x3"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "inception_5a/3x3/scale1"
  type: "Scale"
  bottom: "inception_5a/3x3"
  top: "inception_5a/3x3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "inception_5a/3x3/relu1"
  type: "ReLU"
  bottom: "inception_5a/3x3"
  top: "inception_5a/3x3"
}

layer {
  name: "inception_5a/output"
  type: "Concat"
  bottom: "inception_5a/1x1"
  bottom: "inception_5a/3x3"
  top: "inception_5a/output"
}


########################################################################
# L6 MiniGoogLeNet.inception_module(x, 96, 64, chanDim)
########################################################################
layer {
  name: "inception_6a/1x1"
  type: "Convolution"
  bottom: "inception_5a/output"
  top: "inception_6a/1x1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 1
    weight_filler {
      type: "xavier"
      std: 0.03
    }
    bias_filler {
      type: "constant"
      value: 0.2
    }
  }
}
layer {
  name: "inception_6a/1x1/bn1"
  type: "BatchNorm"
  bottom: "inception_6a/1x1"
  top: "inception_6a/1x1"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "inception_6a/1x1/scale1"
  type: "Scale"
  bottom: "inception_6a/1x1"
  top: "inception_6a/1x1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "inception_6a/1x1/relu1"
  type: "ReLU"
  bottom: "inception_6a/1x1"
  top: "inception_6a/1x1"
}

layer {
  name: "inception_6a/3x3"
  type: "Convolution"
  bottom: "inception_5a/output"
  top: "inception_6a/3x3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    kernel_size: 3
    pad: 1
    weight_filler {
      type: "xavier"
      std: 0.03
    }
    bias_filler {
      type: "constant"
      value: 0.2
    }
  }
}
layer {
  name: "inception_6a/3x3/bn1"
  type: "BatchNorm"
  bottom: "inception_6a/3x3"
  top: "inception_6a/3x3"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "inception_6a/3x3/scale1"
  type: "Scale"
  bottom: "inception_6a/3x3"
  top: "inception_6a/3x3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "inception_6a/3x3/relu1"
  type: "ReLU"
  bottom: "inception_6a/3x3"
  top: "inception_6a/3x3"
}

layer {
  name: "inception_6a/output"
  type: "Concat"
  bottom: "inception_6a/1x1"
  bottom: "inception_6a/3x3"
  top: "inception_6a/output"
}

########################################################################
# L7 MiniGoogLeNet.inception_module(x, 80, 80, chanDim)
########################################################################
layer {
  name: "inception_7a/1x1"
  type: "Convolution"
  bottom: "inception_6a/output"
  top: "inception_7a/1x1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 80
    kernel_size: 1
    weight_filler {
      type: "xavier"
      std: 0.03
    }
    bias_filler {
      type: "constant"
      value: 0.2
    }
  }
}
layer {
  name: "inception_7a/1x1/bn1"
  type: "BatchNorm"
  bottom: "inception_7a/1x1"
  top: "inception_7a/1x1"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "inception_7a/1x1/scale1"
  type: "Scale"
  bottom: "inception_7a/1x1"
  top: "inception_7a/1x1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "inception_7a/1x1/relu1"
  type: "ReLU"
  bottom: "inception_7a/1x1"
  top: "inception_7a/1x1"
}

layer {
  name: "inception_7a/3x3"
  type: "Convolution"
  bottom: "inception_6a/output"
  top: "inception_7a/3x3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 80
    kernel_size: 3
    pad: 1
    weight_filler {
      type: "xavier"
      std: 0.03
    }
    bias_filler {
      type: "constant"
      value: 0.2
    }
  }
}
layer {
  name: "inception_7a/3x3/bn1"
  type: "BatchNorm"
  bottom: "inception_7a/3x3"
  top: "inception_7a/3x3"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "inception_7a/3x3/scale1"
  type: "Scale"
  bottom: "inception_7a/3x3"
  top: "inception_7a/3x3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "inception_7a/3x3/relu1"
  type: "ReLU"
  bottom: "inception_7a/3x3"
  top: "inception_7a/3x3"
}

layer {
  name: "inception_7a/output"
  type: "Concat"
  bottom: "inception_7a/1x1"
  bottom: "inception_7a/3x3"
  top: "inception_7a/output"
}

########################################################################
# L8 MiniGoogLeNet.inception_module(x, 48, 96, chanDim)
########################################################################
layer {
  name: "inception_8a/1x1"
  type: "Convolution"
  bottom: "inception_7a/output"
  top: "inception_8a/1x1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 48
    kernel_size: 1
    weight_filler {
      type: "xavier"
      std: 0.03
    }
    bias_filler {
      type: "constant"
      value: 0.2
    }
  }
}
layer {
  name: "inception_8a/1x1/bn1"
  type: "BatchNorm"
  bottom: "inception_8a/1x1"
  top: "inception_8a/1x1"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "inception_8a/1x1/scale1"
  type: "Scale"
  bottom: "inception_8a/1x1"
  top: "inception_8a/1x1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "inception_8a/1x1/relu1"
  type: "ReLU"
  bottom: "inception_8a/1x1"
  top: "inception_8a/1x1"
}

layer {
  name: "inception_8a/3x3"
  type: "Convolution"
  bottom: "inception_7a/output"
  top: "inception_8a/3x3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 3
    pad: 1
    weight_filler {
      type: "xavier"
      std: 0.03
    }
    bias_filler {
      type: "constant"
      value: 0.2
    }
  }
}
layer {
  name: "inception_8a/3x3/bn1"
  type: "BatchNorm"
  bottom: "inception_8a/3x3"
  top: "inception_8a/3x3"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "inception_8a/3x3/scale1"
  type: "Scale"
  bottom: "inception_8a/3x3"
  top: "inception_8a/3x3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "inception_8a/3x3/relu1"
  type: "ReLU"
  bottom: "inception_8a/3x3"
  top: "inception_8a/3x3"
}

layer {
  name: "inception_8a/output"
  type: "Concat"
  bottom: "inception_8a/1x1"
  bottom: "inception_8a/3x3"
  top: "inception_8a/output"
}

########################################################################
# downsample module
#L9 MiniGoogLeNet.downsample_module(x, 96, chanDim)
########################################################################

layer {
  name: "downsample_9/3x3_s2"
  type: "Convolution"
  bottom: "inception_8a/output"
  top: "downsample_9/3x3_s2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    kernel_size: 3
    stride:2
    pad: 1
    weight_filler {
      type: "xavier"
      std: 0.03
    }
    bias_filler {
      type: "constant"
      value: 0.2
    }
  }
}
layer {
  name: "downsample_9/3x3_s2/bn1"
  type: "BatchNorm"
  bottom: "downsample_9/3x3_s2"
  top: "downsample_9/3x3_s2"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "downsample_9/3x3_s2/scale1"
  type: "Scale"
  bottom: "downsample_9/3x3_s2"
  top: "downsample_9/3x3_s2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "downsample_9/3x3_s2/relu1"
  type: "ReLU"
  bottom: "downsample_9/3x3_s2"
  top: "downsample_9/3x3_s2"
}
layer {
  name: "downsample_9/pool_s2"
  type: "Pooling"
  bottom: "inception_8a/output"
  top: "downsample_9/pool_s2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
    #pad: 1
  }
}
layer {
  name: "downsample_9/output"
  type: "Concat"
  bottom: "downsample_9/3x3_s2"
  bottom: "downsample_9/pool_s2"
  top: "downsample_9/output"
}

########################################################################
# L10 MiniGoogLeNet.inception_module(x, 176, 160, chanDim)
########################################################################
layer {
  name: "inception_10a/1x1"
  type: "Convolution"
  bottom: "downsample_9/output"
  top: "inception_10a/1x1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 176
    kernel_size: 1
    weight_filler {
      type: "xavier"
      std: 0.03
    }
    bias_filler {
      type: "constant"
      value: 0.2
    }
  }
}
layer {
  name: "inception_10a/1x1/bn1"
  type: "BatchNorm"
  bottom: "inception_10a/1x1"
  top: "inception_10a/1x1"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "inception_10a/1x1/scale1"
  type: "Scale"
  bottom: "inception_10a/1x1"
  top: "inception_10a/1x1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "inception_10a/1x1/relu1"
  type: "ReLU"
  bottom: "inception_10a/1x1"
  top: "inception_10a/1x1"
}

layer {
  name: "inception_10a/3x3"
  type: "Convolution"
  bottom: "downsample_9/output"
  top: "inception_10a/3x3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 160
    kernel_size: 3
    pad: 1
    weight_filler {
      type: "xavier"
      std: 0.03
    }
    bias_filler {
      type: "constant"
      value: 0.2
    }
  }
}
layer {
  name: "inception_10a/3x3/bn1"
  type: "BatchNorm"
  bottom: "inception_10a/3x3"
  top: "inception_10a/3x3"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "inception_10a/3x3/scale1"
  type: "Scale"
  bottom: "inception_10a/3x3"
  top: "inception_10a/3x3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "inception_10a/3x3/relu1"
  type: "ReLU"
  bottom: "inception_10a/3x3"
  top: "inception_10a/3x3"
}

layer {
  name: "inception_10a/output"
  type: "Concat"
  bottom: "inception_10a/1x1"
  bottom: "inception_10a/3x3"
  top: "inception_10a/output"
}

########################################################################
# L11 MiniGoogLeNet.inception_module(x, 176, 160, chanDim)
########################################################################
layer {
  name: "inception_11a/1x1"
  type: "Convolution"
  bottom: "inception_10a/output"
  top: "inception_11a/1x1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 176
    kernel_size: 1
    weight_filler {
      type: "xavier"
      std: 0.03
    }
    bias_filler {
      type: "constant"
      value: 0.2
    }
  }
}
layer {
  name: "inception_11a/1x1/bn1"
  type: "BatchNorm"
  bottom: "inception_11a/1x1"
  top: "inception_11a/1x1"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "inception_11a/1x1/scale1"
  type: "Scale"
  bottom: "inception_11a/1x1"
  top: "inception_11a/1x1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "inception_11a/1x1/relu1"
  type: "ReLU"
  bottom: "inception_11a/1x1"
  top: "inception_11a/1x1"
}

layer {
  name: "inception_11a/3x3"
  type: "Convolution"
  bottom: "inception_10a/output"
  top: "inception_11a/3x3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 160
    kernel_size: 3
    pad: 1
    weight_filler {
      type: "xavier"
      std: 0.03
    }
    bias_filler {
      type: "constant"
      value: 0.2
    }
  }
}
layer {
  name: "inception_11a/3x3/bn1"
  type: "BatchNorm"
  bottom: "inception_11a/3x3"
  top: "inception_11a/3x3"
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  param {
    lr_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "inception_11a/3x3/scale1"
  type: "Scale"
  bottom: "inception_11a/3x3"
  top: "inception_11a/3x3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "inception_11a/3x3/relu1"
  type: "ReLU"
  bottom: "inception_11a/3x3"
  top: "inception_11a/3x3"
}

layer {
  name: "inception_11a/output"
  type: "Concat"
  bottom: "inception_11a/1x1"
  bottom: "inception_11a/3x3"
  top: "inception_11a/output"
}

########################################################################
# L12 + L13:  x=AveragePooling2D(7,7)(x) x=Dropout(0.5)(x)
########################################################################
layer {
  name: "avg_pool_12/8x8_s1"
  type: "Pooling"
  bottom: "inception_11a/output"
  top: "avg_pool_12/8x8_s1"
  pooling_param {
    pool: AVE
    kernel_size: 8
    stride: 1
  }
}
layer {
  name: "drop_8x8_s1"
  type: "Dropout"
  bottom: "avg_pool_12/8x8_s1"
  top: "avg_pool_12/8x8_s1"
  dropout_param {
    dropout_ratio: 0.5
  }
}

########################################################################
# L14 + L16:  Fullly Connected and SoftMax Classifier
########################################################################

layer {
  name: "latent"
  type: "InnerProduct"
  bottom: "avg_pool_12/8x8_s1"
  top: "latent"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 48
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  bottom: "latent"
  top: "latent_sigmoid"
  name: "latent_sigmoid"
  type: "Sigmoid"
}
layer {
  bottom: "latent_sigmoid"
  bottom: "latent_sigmoid"
  name: "loss_1"
  type: "K1_EuclideanLoss"
  propagate_down: 1
  propagate_down: 0
  top: "loss: forcing-binary"
  loss_weight: 1
}
layer {
  name: "latent_sigmoid_reshape"
  type: "Reshape"
  bottom: "latent_sigmoid"
  top: "latent_sigmoid_reshape"
  reshape_param {
    shape {
      dim: 0  # copy the dimension from below
      dim: 1
      dim: 1
      dim: -1 # infer it from the other dimensions
    }
  }
}
layer {
  name: "latent_sigmoid_avg"
  type: "Pooling"
  bottom: "latent_sigmoid_reshape"
  top: "latent_sigmoid_avg"
  pooling_param {
    pool: AVE
    kernel_h: 1
    kernel_w: 48
  }
}
layer {
  bottom: "latent_sigmoid_avg"
  bottom: "latent_sigmoid_avg"
  name: "loss_2"
  type: "K2_EuclideanLoss"
  propagate_down: 1
  propagate_down: 0
  top: "loss: 50%-fire-rate"
  loss_weight: 1
}

########################################################################

layer {
  name: "fc10"
  type: "InnerProduct"
  bottom: "latent_sigmoid"
  top: "fc10"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
#layer {
#  name: "prob"
#  type: "Softmax"
#  bottom: "loss/classifier"
#  top: "prob"
#}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc10"
  bottom: "label"
  top: "loss"
}
layer {
  name: "accuracy-top1"
  type: "Accuracy"
  bottom: "fc10"
  bottom: "label"
  top: "top-1"
  include {
    phase: TEST
  }
}
layer {
  name: "accuracy-top5"
  type: "Accuracy"
  bottom: "fc10"
  bottom: "label"
  top: "top-5"
  include {
    phase: TEST
  }
  accuracy_param {
    top_k: 5
  }
}
